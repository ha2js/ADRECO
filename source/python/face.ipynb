{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileFaceGaze model...\n",
      "Model loaded using cuda:0 as device\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "333\n",
      "xxxx\n",
      "????\n",
      "222\n",
      "에러\n"
     ]
    }
   ],
   "source": [
    "# %load demo.py\n",
    "import face_recognition\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easydict\n",
    "from tensorflow.keras.utils import get_file\n",
    "from contextlib import contextmanager\n",
    "from omegaconf import OmegaConf\n",
    "from src.factory import get_model\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import utils_sp as utils\n",
    "from models import gazenet\n",
    "from mtcnn import FaceDetector\n",
    "import pymysql\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import random\n",
    "\n",
    "pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.6/EfficientNetB3_224_weights.11-3.44.hdf5\"\n",
    "#연령,성별 예측모델 로드\n",
    "modhash = '6d7f7b7ced093a8b3ef6399163da6ece'\n",
    "# url1 = \"http://192.168.0.16:3001/Ads_img1\"\n",
    "# url2 = \"http://192.168.0.16:3001/check1\"\n",
    "conn = pymysql.connect(host='localhost', user='ssafy', password='ssafy', db='adreco', charset='utf8')\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX, #얼굴 인식시\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED) # 네모낳게 얼굴 테두리 선으로 표시\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA) #성별,연령 M,F 20\n",
    "\n",
    "@contextmanager\n",
    "def video_capture(num): # 카메라 키는거 \n",
    "    cap = cv2.VideoCapture(num)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def yield_images():\n",
    "    # capture video\n",
    "    with video_capture(0) as cap:\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640) # 카메라 읽는거\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480) # 카메라 읽는거\n",
    "        \n",
    "        while True:\n",
    "            # get video frame\n",
    "            ret, img = cap.read()\n",
    "            print(\"xxxx\")\n",
    "\n",
    "            if not ret:\n",
    "                raise RuntimeError(\"Failed to capture image\")\n",
    "\n",
    "            yield img\n",
    "            \n",
    "def db_insert_clear(ad_num, dic, known_face_encodings, known_face_names): # 20초동안 축적된 정보를 디비에 올리고 비움.\n",
    "    curs = conn.cursor()\n",
    "    sql = \"INSERT INTO user_info VALUES(NOW(), %s, %s, %s, %s, %s)\"\n",
    "    for k in dic:\n",
    "        if dic[k]['seetime'] != 0.0:\n",
    "            data = (str(ad_num), k, int(dic[k]['age']/10)*10, dic[k]['gender'], dic[k]['seetime'])\n",
    "            curs.execute(sql, data)\n",
    "    conn.commit()\n",
    "    dic.clear() # 20초동안 들어간 정보 배열 초기화\n",
    "    known_face_encodings.clear() # 20초안에 들어온 사용자 인코딩 정보 저장된거 삭제\n",
    "    known_face_names.clear()     # 얘도 뭐 저장같이 되는건데 삭제\n",
    "    files = os.listdir('knowns')\n",
    "    for filename in files:  #얘는 인코딩된 사진들 knowns 폴더에 저장되는데 20초 지나면 전부 삭제\n",
    "        os.remove('knowns/'+filename)\n",
    "\n",
    "def main():\n",
    "    # 시선 추적 모델 로드\n",
    "    args_gaze = easydict.EasyDict({\n",
    "        \"cpu\" : None,\n",
    "        \"weights\" : 'models/weights/gazenet.pth' #학습완료 모델 가중치 로드하는것.\n",
    "    })\n",
    "    print('Loading MobileFaceGaze model...')\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and not args_gaze.cpu) else \"cpu\")\n",
    "    # gpu 사용하려면 cuda 사용해야함. 안되면 cpu 사용되는 조건문\n",
    "    gaze_model = gazenet.GazeNet(device) #시선추적 때 사용할 모델 지정.\n",
    "\n",
    "    if(not torch.cuda.is_available() and not args_gaze.cpu):\n",
    "        print('Tried to load GPU but found none. Please check your environment')\n",
    "\n",
    "    state_dict = torch.load(args_gaze.weights, map_location=device)\n",
    "    gaze_model.load_state_dict(state_dict) # 여기서 시선추적 모델 로드 됨.\n",
    "    print('Model loaded using {} as device'.format(device))\n",
    "    \n",
    "    gaze_model.eval()\n",
    "    \n",
    "    timecount = 0.0 # 20초 세는 변수\n",
    "    margin = 0.4\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    person_num = 0 # 인식 사람 번호\n",
    "    dirname = 'knowns' # 파일명\n",
    "    dic = {} #배열 같은것.\n",
    "    frame_count = 0 #프레임 속도 잼.\n",
    "    \n",
    "    \n",
    "    curs = conn.cursor()\n",
    "#     sql = \"SELECT MAX(ad_num) FROM ad_log\" # 몇번 째 광고인지\n",
    "#     curs.execute(sql)\n",
    "#     rows = curs.fetchone() \n",
    "    \n",
    "    \n",
    "#     if rows[0] == None:\n",
    "#         ad_num = 1\n",
    "#     else: ad_num = rows[0]+1 # 광고번호\n",
    "    \n",
    "    ad_num = 1\n",
    "\n",
    "    face_detector = FaceDetector(device=device) # 얼굴 디텍터(검출)를 변수에 저장\n",
    "    \n",
    "    weight_file = get_file(\"EfficientNetB3_224_weights.11-3.44.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "                               file_hash=modhash, cache_dir=str(Path(\"__file__\").resolve().parent))\n",
    "    #가중치 파일을 가지고 오고, \n",
    "    \n",
    "    # for face detection\n",
    "    detector = dlib.get_frontal_face_detector() #face reconigation 패키지에 있는 디텍터\n",
    "    \n",
    "    # load model and weights\n",
    "    model_name, img_size = Path(weight_file).stem.split(\"_\")[:2]\n",
    "    img_size = int(img_size)\n",
    "    cfg = OmegaConf.from_dotlist([f\"model.model_name={model_name}\", f\"model.img_size={img_size}\"])\n",
    "    model = get_model(cfg)\n",
    "    model.load_weights(weight_file) # 여기까지 하면 모델 생성 완료\n",
    "    \n",
    "    image_generator = yield_images() # 영상읽은거\n",
    "    \n",
    "    for img in image_generator: # img 변수에 한프레임 담김.\n",
    "        print(\"????\")\n",
    "        timer = time.time() # 현재시각\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGB 변환\n",
    "        img_h, img_w, _ = np.shape(input_img) # 이미지 높이,너비 가져옴\n",
    "        print(\"222\")\n",
    "        # detect faces using dlib detector\n",
    "        detected = detector(input_img, 1) # 디텍팅\n",
    "        gaze_detected, landmarks = face_detector.detect(Image.fromarray(input_img)) # gaze_detected는 안씀,landmarks:특징점\n",
    "        \n",
    "        faces = np.empty((len(detected), img_size, img_size, 3)) # len은 식별된 사람 수, 얼굴들의 사이즈 * 사이즈만큼 빈 행렬을 잡고 \n",
    "                                                                 # 그 정보들을 faces라는 변수에 넣을 예정\n",
    "        count_age_gender = [\n",
    "                0, 0, 0, 0, 0, 0, #남자 10 ~ 60 각 나이대별 사람수 세려고 0으로 싹다 초기값.\n",
    "                0, 0, 0, 0, 0, 0 #여자 10 ~ 60\n",
    "            ]\n",
    "        \n",
    "        if len(detected) > 0:\n",
    "            face_names = []\n",
    "            z_length = []\n",
    "            for i, (d, lm) in enumerate(zip(detected, landmarks)): # ex 5명식별시 1명씩 처리하는 for문\n",
    "                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height() # 얼굴 하나의 좌표\n",
    "                xw1 = max(int(x1 - margin * w), 0)\n",
    "                yw1 = max(int(y1 - margin * h), 0)\n",
    "                xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "                yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2) # 얼굴에 네모 그림\n",
    "                faces[i] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1], (img_size, img_size)) # faces에 디텍팅된 얼굴들 크기가 다\n",
    "                                                                                        #다르므로 같은 크기로 맞춰줌.\n",
    "\n",
    "                #얼굴 인식(번호 매기기)\n",
    "                face_encoding = face_recognition.face_encodings(input_img, [(y1, x1, y2, x2)])[0] # input_img : 전체 화면\n",
    "                                                                                            #이 전체 화면에 y1,x1,y2,x2 좌표를 encoding\n",
    "                if len(known_face_encodings) == 0: # 우리가 알고 있는 얼굴들의 배열에 값이 없을 때\n",
    "                    min_value = 1.0 # 1.0이 최대값. 이걸로 초기화\n",
    "                else: # 아는 얼굴이 있을 때\n",
    "                    distances = face_recognition.face_distance(known_face_encodings, face_encoding) # 내가 아는 얼굴들과 새로 인식된 얼굴들을\n",
    "                                                                                                    # 검사해서 distance를 구해서 같은사람인지 확인\n",
    "                    min_value = min(distances) # distance가 작을 수록 비슷한 사람이다라는 뜻.\n",
    "\n",
    "                name = \"Unknown\"\n",
    "                if min_value < 0.30: # 위에서 구한 distance의 최소값인 min_value가 0.30 이하 일때 같은 사람으로 판단.\n",
    "                    index = np.argmin(distances) # 몇번째 사람인지\n",
    "                    name = known_face_names[index] # 그사람의 번호를 이름으로 사용\n",
    "                    \n",
    "                else: # unkowns 일때 사진 저장\n",
    "                    name = str(person_num) # 초기값이라면 0임 . 0이라는 이름의 사람이라는뜻.\n",
    "                    person_num += 1 # 다음 사람을 위해 +1\n",
    "                    \n",
    "                    # 얼굴 검출 저장\n",
    "                    detected_face_img = Image.fromarray(input_img[yw1:yw2, xw1:xw2]) # input_img라는 전체 화면에서 해당 좌표만큼 짤라서\n",
    "                                                                                     # 이미지 저장.\n",
    "                    detected_face_img.save('knowns/'+name+'.jpg')\n",
    "                    known_face_names.append(name) # 이미 알고 있는 배열에 이름(사람 번호) 추가.\n",
    "                    known_face_encodings.append(face_encoding) # 인코딩도 추가\n",
    "                    \n",
    "                # Crop and normalize face Face\n",
    "                gaze_face, gaze_origin, M  = utils.normalize_face(lm, input_img) # 랜드마크(특징점), 전체 화면\n",
    "\n",
    "                # Predict gaze\n",
    "                with torch.no_grad():\n",
    "                    gaze = gaze_model.get_gaze(gaze_face)\n",
    "                    gaze = gaze[0].data.cpu()   # 시선추적은 cpu사용함.                         \n",
    "                print(\"333\")\n",
    "                # Draw results\n",
    "                img = cv2.circle(img, gaze_origin, 3, (0, 255, 0), -1) # circle 미간의 점.\n",
    "                img = utils.draw_gaze(img, gaze_origin, gaze, color=(255,0,0), thickness=2) # draw_gaze : 화살표그린 것.\n",
    "\n",
    "                z_length.append(utils.zlength(gaze)) # z_length : 화살표 길이\n",
    "                face_names.append(name) # 인식된 사람 이름 넣기.\n",
    "                if not name in dic.keys():\n",
    "                    dic[name] = {} # 4번째사람까지 저장되있는데 만약 name이 5라면 초기화 해주고\n",
    "                    dic[name]['seetime'] = 0.0 # 쳐다본 시간을 0 으로 초기화\n",
    "            \n",
    "            # predict ages and genders of the detected faces\n",
    "            results = model.predict(faces) # ex)5명 resize해놓은것을 넣어서 각각의 예측 결과를 뽑음.\n",
    "            predicted_genders = results[0] # 0번인덱스 : 성별\n",
    "            ages = np.arange(0, 101).reshape(101, 1) # 뒤집기\n",
    "            predicted_ages = results[1].dot(ages).flatten()\n",
    "            # results[1] : 나이  / dot:행렬곱  / faltten: 2차원배열 -> 1차원으로 변경\n",
    "            \n",
    "            for (i, d), name in zip(enumerate(detected), face_names):\n",
    "                if not 'age' in dic[name].keys(): #이미 나이를 알고 있다면 안돌게함.\n",
    "                    age = int(predicted_ages[i])\n",
    "                    gender =  \"M\" if predicted_genders[i][0] < 0.5 else \"F\"\n",
    "                    if 10 <= age and age <= 69:\n",
    "                        if gender == 'M': count_age_gender[0 + (int(age/10)-1)] += 1 # 각 나이대별 count증가\n",
    "                        else: count_age_gender[6 + (int(age/10)-1)] += 1\n",
    "                            \n",
    "                    dic[name]['age'], dic[name]['gender'] = age, gender\n",
    "                label = \"{}, {}, {}\".format(name, dic[name]['age'], dic[name]['gender']) # 식별사용자 번호, 나이, 성별 라벨로 만듬\n",
    "                draw_label(img, (d.left(), d.top()), label) #카메라에 담기는 화면 전체 이미지 left,top 만큼 옮긴 좌표에 그림,만든 라벨\n",
    "            if frame_count == 0: # 첫 프레임일 때\n",
    "                m = max(count_age_gender)\n",
    "                m_list = [i for i, j in enumerate(count_age_gender) if j == m]\n",
    "                choice = random.choice(m_list) # 10대 10명,30대 10명으로 가장 많다면? 둘중에 랜덤으로 뽑음.\n",
    "                x, y = choice%6, int(choice/6) # 몇번째 열이고 행인지 나옴\n",
    "                data2 = { 'check_req': True } # 첫번째 프레임일때 광고가 바뀔 수 있게 true로 해둠.\n",
    "                data1 = {\n",
    "                    'age': 0 if not count_age_gender else (x+1)*10, \n",
    "                    'gender': '0' if not count_age_gender else (\"M\" if y == 0 else \"F\")\n",
    "                }\n",
    "                #requests.post(url2, json=data2) #true로 넘어가면 광고 바뀔때다.\n",
    "                #requests.post(url1, json=data1) # 10, F 넘어감.\n",
    "\n",
    "            addtime =  time.time() - timer # 한 프레임당 시간  여기서 timer은 위에 시작할 때 현재시간 찍어둠.\n",
    "            for i in range(len(z_length)):\n",
    "                if z_length[i] <= 25: # 거리가 25 이하면 쳐다보고 있다고 가정\n",
    "                    dic[face_names[i]]['seetime'] += addtime # 쳐다본 시간(프레임 시간) 만큼 더함.\n",
    "                img = cv2.putText(img, '{} : {:.2f}'.format(face_names[i], dic[face_names[i]]['seetime']), (0, 40+20*i), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)    \n",
    "                        #카메라 나오면 화면 왼쪽 상단에 표시되는 사용자 이름(번호), 좌표, 쳐다본시간\n",
    "        else:\n",
    "            if frame_count == 0:\n",
    "                data2 = { 'check_req': True }\n",
    "                data1 = {\n",
    "                    'age': 0, \n",
    "                    'gender': '0'\n",
    "                }\n",
    "                #requests.post(url2, json=data2)\n",
    "                #requests.post(url1, json=data1)\n",
    "            addtime =  time.time() - timer\n",
    "        timecount += addtime;\n",
    "        img = cv2.putText(img, 'TIME: {:.2f}'.format(timecount), (0, 20), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA) \n",
    "        frame_count += 1\n",
    "        if int(timecount) > 20:\n",
    "            timecount = 0.0\n",
    "            person_num = 0\n",
    "            db_insert_clear(ad_num, dic, known_face_encodings, known_face_names)\n",
    "            ad_num += 1\n",
    "            frame_count = 0\n",
    "            \n",
    "        cv2.imshow(\"result\", img)\n",
    "        key = cv2.waitKey(30)\n",
    "            \n",
    "        if key == 27:  # ESC\n",
    "            print(\"ESC\", dic)\n",
    "            break\n",
    "            \n",
    "    conn.close()\n",
    "    cv2.destroyAllWindows() # 창 닫힘.\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except:\n",
    "        print(\"에러\")\n",
    "        conn.close()\n",
    "        cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The conda.compat module is deprecated and will be removed in a future release.\n",
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.11\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/heung/anaconda3/envs/project\n",
      "\n",
      "  added / updated specs:\n",
      "    - opencv3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    opencv3-3.1.0              |           py36_0        37.4 MB  menpo\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        37.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  opencv3            menpo/linux-64::opencv3-3.1.0-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "opencv3-3.1.0        | 37.4 MB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c menpo opencv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "count_age = [0, 0, 0, 0, 0, 0] # 10 ~ 60\n",
    "count_gender = [5, 4] # M, F\n",
    "print((count_age.index(max(count_age))+1)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8965996722408499740,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12504568892306683093\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1113325568\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17662421527034369496\n",
       " physical_device_desc: \"device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 4177782695382151827\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 05 13:14:41 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.09       Driver Version: 457.09       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX150      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   35C    P3    N/A /  N/A |     64MiB /  2048MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fdcdc12aa0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDLIB_USE_CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "dlib.DLIB_USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url1 = \"http://172.30.1.53:3001/Ads_img1\"\n",
    "url2 = \"http://172.30.1.53:3001/check1\"\n",
    "data2 = {'check_req': True}\n",
    "data1 = {'age': 10, 'gender': 'M'}\n",
    "requests.post(url2, json=data2)\n",
    "requests.post(url1, json=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "conn = pymysql.connect(host='localhost', user='root', password='1234', db='swp', charset='utf8')\n",
    "curs = conn.cursor()\n",
    "sql1 = \"SELECT MAX(학번) FROM test1\"\n",
    "sql2 = \"SELECT MAX(ad_num) FROM ad_log\"\n",
    "curs.execute(sql2)\n",
    "rows = curs.fetchone()\n",
    "if rows[0] == None:\n",
    "    ad_num = 1\n",
    "else: ad_num = rows[0]+1\n",
    "print(ad_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "40\n",
      "남자\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "count = [\n",
    "    7, 0, 4, 9, 2, 9, #남자 10 ~ 60\n",
    "    9, 1, 6, 5, 9, 0 #여자 10 ~ 60\n",
    "]\n",
    "m = count.index(max(count))\n",
    "print(m)\n",
    "x, y = m%6, int(m/6)\n",
    "print(y)\n",
    "print((x+1)*10)\n",
    "print(\"남자\" if y == 0 else \"F\")\n",
    "\n",
    "import random\n",
    "m = max(count)\n",
    "m_list = [i for i, j in enumerate(count) if j == m]\n",
    "print(random.choice(m_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook face.ipynb to script\n",
      "[NbConvertApp] Writing 11108 bytes to face.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script face.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MobileFaceGaze model...\n",
      "Model loaded using cuda:0 as device\n",
      "ESC {'0': {'seetime': 1.1100289821624756, 'age': 24, 'gender': 'M'}}\n"
     ]
    }
   ],
   "source": [
    "# %load demo.py\n",
    "import face_recognition\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import easydict\n",
    "from tensorflow.keras.utils import get_file\n",
    "from contextlib import contextmanager\n",
    "from omegaconf import OmegaConf\n",
    "from src.factory import get_model\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import utils_sp as utils\n",
    "from models import gazenet\n",
    "from mtcnn import FaceDetector\n",
    "import pymysql\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import random\n",
    "\n",
    "pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.6/EfficientNetB3_224_weights.11-3.44.hdf5\"\n",
    "modhash = '6d7f7b7ced093a8b3ef6399163da6ece'\n",
    "url1 = \"http://192.168.0.16:3001/Ads_img1\"\n",
    "url2 = \"http://192.168.0.16:3001/check1\"\n",
    "conn = pymysql.connect(host='localhost', user='root', password='1234', db='swp', charset='utf8')\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "@contextmanager\n",
    "def video_capture(*args, **kwargs):\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "def yield_images():\n",
    "    # capture video\n",
    "    with video_capture(1) as cap:\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        while True:\n",
    "            # get video frame\n",
    "            ret, img = cap.read()\n",
    "\n",
    "            if not ret:\n",
    "                raise RuntimeError(\"Failed to capture image\")\n",
    "\n",
    "            yield img\n",
    "            \n",
    "def db_insert_clear(ad_num, dic, known_face_encodings, known_face_names):\n",
    "    curs = conn.cursor()\n",
    "    sql = \"INSERT INTO user_info VALUES(NOW(), %s, %s, %s, %s, %s)\"\n",
    "    for k in dic:\n",
    "        if dic[k]['seetime'] != 0.0:\n",
    "            data = (str(ad_num), k, int(dic[k]['age']/10)*10, dic[k]['gender'], dic[k]['seetime'])\n",
    "            curs.execute(sql, data)\n",
    "    conn.commit()\n",
    "    dic.clear()\n",
    "    known_face_encodings.clear()\n",
    "    known_face_names.clear()\n",
    "    files = os.listdir('knowns')\n",
    "    for filename in files:\n",
    "        os.remove('knowns/'+filename)\n",
    "\n",
    "def main():\n",
    "    # 시선 추적 모델 로드\n",
    "    args_gaze = easydict.EasyDict({\n",
    "        \"cpu\" : None,\n",
    "        \"weights\" : 'models/weights/gazenet.pth'\n",
    "    })\n",
    "    print('Loading MobileFaceGaze model...')\n",
    "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and not args_gaze.cpu) else \"cpu\")\n",
    "    gaze_model = gazenet.GazeNet(device)\n",
    "\n",
    "    if(not torch.cuda.is_available() and not args_gaze.cpu):\n",
    "        print('Tried to load GPU but found none. Please check your environment')\n",
    "\n",
    "    state_dict = torch.load(args_gaze.weights, map_location=device)\n",
    "    gaze_model.load_state_dict(state_dict)\n",
    "    print('Model loaded using {} as device'.format(device))\n",
    "    \n",
    "    gaze_model.eval()\n",
    "    \n",
    "    timecount = 0.0\n",
    "    margin = 0.4\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "    person_num = 0\n",
    "    dirname = 'knowns'\n",
    "    dic = {}\n",
    "    frame_count = 0\n",
    "    \n",
    "    curs = conn.cursor()\n",
    "    sql = \"SELECT MAX(ad_num) FROM ad_log\"\n",
    "    curs.execute(sql)\n",
    "    rows = curs.fetchone()\n",
    "    if rows[0] == None:\n",
    "        ad_num = 1\n",
    "    else: ad_num = rows[0]+1\n",
    "\n",
    "    face_detector = FaceDetector(device=device)\n",
    "    \n",
    "    weight_file = get_file(\"EfficientNetB3_224_weights.11-3.44.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "                               file_hash=modhash, cache_dir=str(Path(\"__file__\").resolve().parent))\n",
    "\n",
    "    # for face detection\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    # load model and weights\n",
    "    model_name, img_size = Path(weight_file).stem.split(\"_\")[:2]\n",
    "    img_size = int(img_size)\n",
    "    cfg = OmegaConf.from_dotlist([f\"model.model_name={model_name}\", f\"model.img_size={img_size}\"])\n",
    "    model = get_model(cfg)\n",
    "    model.load_weights(weight_file)\n",
    "    \n",
    "    image_generator = yield_images()\n",
    "    \n",
    "    for img in image_generator:\n",
    "        timer = time.time()\n",
    "        input_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_h, img_w, _ = np.shape(input_img)\n",
    "        \n",
    "        # detect faces using dlib detector\n",
    "        detected = detector(input_img, 1)\n",
    "        gaze_detected, landmarks = face_detector.detect(Image.fromarray(input_img))\n",
    "        \n",
    "        faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "        count_age_gender = [\n",
    "                0, 0, 0, 0, 0, 0, #남자 10 ~ 60\n",
    "                0, 0, 0, 0, 0, 0 #여자 10 ~ 60\n",
    "            ]\n",
    "        \n",
    "        if len(detected) > 0:\n",
    "            face_names = []\n",
    "            z_length = []\n",
    "            for i, (d, lm) in enumerate(zip(detected, landmarks)):\n",
    "                x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "                xw1 = max(int(x1 - margin * w), 0)\n",
    "                yw1 = max(int(y1 - margin * h), 0)\n",
    "                xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "                yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                faces[i] = cv2.resize(img[yw1:yw2 + 1, xw1:xw2 + 1], (img_size, img_size))\n",
    "\n",
    "                #얼굴 인식\n",
    "                face_encoding = face_recognition.face_encodings(input_img, [(y1, x1, y2, x2)])[0]\n",
    "                if len(known_face_encodings) == 0:\n",
    "                    min_value = 1.0\n",
    "                else:\n",
    "                    distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                    min_value = min(distances)\n",
    "\n",
    "                name = \"Unknown\"\n",
    "                if min_value < 0.30:\n",
    "                    index = np.argmin(distances)\n",
    "                    name = known_face_names[index]\n",
    "                    \n",
    "                else: # unkowns 일때 사진 저장\n",
    "                    name = str(person_num)\n",
    "                    person_num += 1\n",
    "                    \n",
    "                    # 얼굴 검출 저장\n",
    "                    detected_face_img = Image.fromarray(input_img[yw1:yw2, xw1:xw2])\n",
    "                    detected_face_img.save('knowns/'+name+'.jpg')\n",
    "                    known_face_names.append(name)\n",
    "                    known_face_encodings.append(face_encoding)\n",
    "                    \n",
    "                # Crop and normalize face Face\n",
    "                gaze_face, gaze_origin, M  = utils.normalize_face(lm, input_img)\n",
    "\n",
    "                # Predict gaze\n",
    "                with torch.no_grad():\n",
    "                    gaze = gaze_model.get_gaze(gaze_face)\n",
    "                    gaze = gaze[0].data.cpu()                              \n",
    "\n",
    "                # Draw results\n",
    "                img = cv2.circle(img, gaze_origin, 3, (0, 255, 0), -1)\n",
    "                img = utils.draw_gaze(img, gaze_origin, gaze, color=(255,0,0), thickness=2)\n",
    "\n",
    "                z_length.append(utils.zlength(gaze))\n",
    "                face_names.append(name)\n",
    "                if not name in dic.keys():\n",
    "                    dic[name] = {}\n",
    "                    dic[name]['seetime'] = 0.0\n",
    "            \n",
    "            # predict ages and genders of the detected faces\n",
    "            results = model.predict(faces)\n",
    "            predicted_genders = results[0]\n",
    "            ages = np.arange(0, 101).reshape(101, 1)\n",
    "            predicted_ages = results[1].dot(ages).flatten()\n",
    "            \n",
    "            for (i, d), name in zip(enumerate(detected), face_names):\n",
    "                if not 'age' in dic[name].keys():\n",
    "                    age = int(predicted_ages[i])\n",
    "                    gender =  \"M\" if predicted_genders[i][0] < 0.5 else \"F\"\n",
    "                    if 10 <= age and age <= 69:\n",
    "                        if gender == 'M': count_age_gender[0 + (int(age/10)-1)] += 1\n",
    "                        else: count_age_gender[6 + (int(age/10)-1)] += 1\n",
    "                            \n",
    "                    dic[name]['age'], dic[name]['gender'] = age, gender\n",
    "                label = \"{}, {}, {}\".format(name, dic[name]['age'], dic[name]['gender'])\n",
    "                draw_label(img, (d.left(), d.top()), label)\n",
    "            if frame_count == 0:\n",
    "                m = max(count_age_gender)\n",
    "                m_list = [i for i, j in enumerate(count_age_gender) if j == m]\n",
    "                choice = random.choice(m_list)\n",
    "                x, y = choice%6, int(choice/6)\n",
    "                data2 = { 'check_req': True }\n",
    "                data1 = {\n",
    "                    'age': 0 if not count_age_gender else (x+1)*10, \n",
    "                    'gender': '0' if not count_age_gender else (\"M\" if y == 0 else \"F\")\n",
    "                }\n",
    "                requests.post(url2, json=data2)\n",
    "                requests.post(url1, json=data1)\n",
    "\n",
    "            addtime =  time.time() - timer\n",
    "            for i in range(len(z_length)):\n",
    "                if z_length[i] <= 25:\n",
    "                    dic[face_names[i]]['seetime'] += addtime\n",
    "                img = cv2.putText(img, '{} : {:.2f}'.format(face_names[i], dic[face_names[i]]['seetime']), (0, 40+20*i), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA)    \n",
    "        else:\n",
    "            if frame_count == 0:\n",
    "                data2 = { 'check_req': True }\n",
    "                data1 = {\n",
    "                    'age': 0, \n",
    "                    'gender': '0'\n",
    "                }\n",
    "                requests.post(url2, json=data2)\n",
    "                requests.post(url1, json=data1)\n",
    "            addtime =  time.time() - timer\n",
    "        timecount += addtime;\n",
    "        img = cv2.putText(img, 'TIME: {:.2f}'.format(timecount), (0, 20), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 1, cv2.LINE_AA) \n",
    "        frame_count += 1\n",
    "        if int(timecount) > 20:\n",
    "            timecount = 0.0\n",
    "            person_num = 0\n",
    "            db_insert_clear(ad_num, dic, known_face_encodings, known_face_names)\n",
    "            ad_num += 1\n",
    "            frame_count = 0\n",
    "            \n",
    "        cv2.imshow(\"result\", img)\n",
    "        key = cv2.waitKey(30)\n",
    "            \n",
    "        if key == 27:  # ESC\n",
    "            print(\"ESC\", dic)\n",
    "            break\n",
    "            \n",
    "    conn.close()\n",
    "    cv2.destroyAllWindows()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except:\n",
    "        conn.close()\n",
    "        cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "count_age = [0, 0, 0, 0, 0, 0] # 10 ~ 60\n",
    "count_gender = [5, 4] # M, F\n",
    "print((count_age.index(max(count_age))+1)*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8965996722408499740,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12504568892306683093\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 1113325568\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17662421527034369496\n",
       " physical_device_desc: \"device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 4177782695382151827\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov 05 13:14:41 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 457.09       Driver Version: 457.09       CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX150      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   35C    P3    N/A /  N/A |     64MiB /  2048MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4fdcdc12aa0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDLIB_USE_CUDA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "dlib.DLIB_USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url1 = \"http://172.30.1.53:3001/Ads_img1\"\n",
    "url2 = \"http://172.30.1.53:3001/check1\"\n",
    "data2 = {'check_req': True}\n",
    "data1 = {'age': 10, 'gender': 'M'}\n",
    "requests.post(url2, json=data2)\n",
    "requests.post(url1, json=data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "conn = pymysql.connect(host='localhost', user='root', password='1234', db='swp', charset='utf8')\n",
    "curs = conn.cursor()\n",
    "sql1 = \"SELECT MAX(학번) FROM test1\"\n",
    "sql2 = \"SELECT MAX(ad_num) FROM ad_log\"\n",
    "curs.execute(sql2)\n",
    "rows = curs.fetchone()\n",
    "if rows[0] == None:\n",
    "    ad_num = 1\n",
    "else: ad_num = rows[0]+1\n",
    "print(ad_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "40\n",
      "남자\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "count = [\n",
    "    7, 0, 4, 9, 2, 9, #남자 10 ~ 60\n",
    "    9, 1, 6, 5, 9, 0 #여자 10 ~ 60\n",
    "]\n",
    "m = count.index(max(count))\n",
    "print(m)\n",
    "x, y = m%6, int(m/6)\n",
    "print(y)\n",
    "print((x+1)*10)\n",
    "print(\"남자\" if y == 0 else \"F\")\n",
    "\n",
    "import random\n",
    "m = max(count)\n",
    "m_list = [i for i, j in enumerate(count) if j == m]\n",
    "print(random.choice(m_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook face.ipynb to script\n",
      "[NbConvertApp] Writing 11108 bytes to face.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script face.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# 시선추적 모델 학습

<br>

## 데이터 셋 준비

1. 정규화된  MPIIFaceGaze 데이터셋 다운

MPIIFaceGaze 데이터 세트는 기반으로 [MPIIGaze의](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild) 추가 인간의 얼굴 랜드 마크 주석으로, 데이터 세트 및 얼굴 영역은 사용 가능한입니다.

448*448 픽셀 크기의 정규화된 얼굴 이미지와 2D 응시 각도 벡터를 포함하는 정규화 된 데이터

2. 준비

```python
for r, d, f in os.walk(DATASET_DIR):
    for file in f:
        if not file.endswith('.mat'): continue
        sid = file.split('.')[0][1:]
        print('Parsing data for subject {}'.format(int(sid)))
        dest_folder = os.path.join(dest, sid)
        if not os.path.exists(dest_folder):os.mkdir(dest_folder)        
        gt_obj = {'subject_id': sid,
                 'images':[],
                 'labels':[]}
        
        with h5py.File(os.path.join(DATASET_DIR, file), 'r') as mat:
            for k, v in mat.items():
                data = np.array(mat['Data']['data'])
                label = np.array(mat['Data']['label'])
            
        for idx, (img, gt) in enumerate(zip(data, label)):
            img_file =  '{:04}.png'.format(idx)
            img_path = os.path.join(dest_folder, img_file)
            cv2.imwrite(img_path, img.transpose(1,2,0))
            gt_obj['images'].append(img_path)
            gt_obj['labels'].append(gt)
            
        pickle_file = os.path.join(dest_folder, 'labels.dict')
        with open(pickle_file, 'wb') as pf:
            pickle.dump(gt_obj, pf)
```

<br>

## config

```yaml
data:
  db: MPIIFaceGaze_normalizad    # 정규화된  MPIIFaceGaze 데이터셋                  

model:
  model_name: mobilenet_v2   	# base model
  img_size: 112                 # input img size

train:
  optimizer_name: sgd
  lr: 0.01 	                    # 학습률
  epochs: 10                    # 에폭
  batch_size: 64                # 배치 사이즈	
```
<br>

## data loader

```python 
train_loader = get_loader(args.dataset, args.batch_size)


class MPIIFaceGazeDataset(torch.utils.data.Dataset):
    def __init__(self, dataset_dir):
        pickle_file = os.path.join(dataset_dir, 'labels.dict')
        with open(pickle_file, 'rb') as pf:
            d = pickle.load(pf)
            self.images = d['images']
            self.labels = d['labels']
            self.sid = d['subject_id']
        

        self.preprocess = transforms.Compose([	# 전처리
            transforms.Resize((112,112)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]) 

    def __getitem__(self, index):
        label = self.labels[index][0:2] # Gaze angles only
        img = Image.open(self.images[index])
        img = self.preprocess(img)
        return img, label
    
    def __len__(self):
        return len(self.images)

    def __repr__(self):
        return self.__class__.__name__

def get_loader(dataset_root, batch_size):
    assert os.path.exists(dataset_root)
    
    train_subjects = [os.path.join(dataset_root, '{:02}').format(i) for i in range(15)]
    train_dataset = torch.utils.data.ConcatDataset([
        MPIIFaceGazeDataset(subject) for subject in train_subjects
    ])
    
    assert len(train_dataset) == 45000

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True,
        num_workers=2
    )
    
    return train_loader
```
<br>

## 데이터 전처리

```python
self.preprocess = transforms.Compose([
    transforms.Resize((112,112)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
]) 
```
<br>

## 모델

```python
model = gazenet.GazeNet(device=device).train()


class GazeNet(nn.Module):

    def __init__(self, device):    
        super(GazeNet, self).__init__()
        self.device = device
        self.preprocess = transforms.Compose([		# 전처리
            transforms.Resize((112,112)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

        model = models.mobilenet_v2(pretrained=True)
        model.features[-1] = models.mobilenet.ConvBNReLU(320, 256, kernel_size=1)
        self.backbone = model.features

        self.Conv1 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0)
        self.Conv2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0)
        self.Conv3 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1, stride=1, padding=0)

        self.fc1 = nn.Sequential(
            nn.Linear(256*4*4, 512),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        self.fc2 = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Dropout(0.5)
        )   
        self.fc_final = nn.Linear(512, 2)

        self._initialize_weight()
        self._initialize_bias()
        self.to(device)


    def _initialize_weight(self):
        nn.init.normal_(self.Conv1.weight, mean=0.0, std=0.01)
        nn.init.normal_(self.Conv2.weight, mean=0.0, std=0.01)
        nn.init.normal_(self.Conv3.weight, mean=0.0, std=0.001)

    def _initialize_bias(self):
        nn.init.constant_(self.Conv1.bias, val=0.1)
        nn.init.constant_(self.Conv2.bias, val=0.1)
        nn.init.constant_(self.Conv3.bias, val=1)

    def forward(self, x):
        
        x = self.backbone(x)
        y = F.relu(self.Conv1(x))
        y = F.relu(self.Conv2(y))
        y = F.relu(self.Conv3(y))
        
        x = F.dropout(F.relu(torch.mul(x, y)), 0.5)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.fc2(x)
        gaze = self.fc_final(x)

        return gaze

    def get_gaze(self, img):
        img = Image.fromarray(img)
        img = self.preprocess(img)[np.newaxis,:,:,:]
        x = self.forward(img.to(self.device))
        return x
```
<br>

## Optimizer 설정

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=args.learning_rate,
    momentum=args.momentum,
    weight_decay=args.weight_decay,
    nesterov=args.nesterov
)
```
<br>

## Scheduler 설정

```python
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[4, 8], gamma=args.lr_decay)
```
<br>

## 학습

```python
start_time = time.time() 
print('Training started at {}'.format(time.strftime("%a, %d %b %Y %H:%M:%S ", time.gmtime())))

for epoch in range(args.epochs):
    for batch_idx, (imgs, gt) in enumerate(train_loader):

        imgs = imgs.float().cuda()
        gt = gt.cuda()

        optimizer.zero_grad()

        outputs = model(imgs)
        loss = loss_fn(outputs, gt)
        loss.backward()

        optimizer.step()
        angle_error = utils.compute_angle_error(outputs, gt).mean()

        if batch_idx % 100 == 0:
            s = ('Epoch {} Step {}/{} '
                        'Loss: {:.4f} '
                        'AngleError: {:.2f}'.format(
                            epoch,
                            batch_idx,
                            len(train_loader),
                            loss.item(),
                            angle_error.item(),
                        ))
            print(s)
            with open(args.log_path, 'a') as f:
                f.write('{}\n'.format(s))

    
    print('epoch finished')
    elapsed = time.time() - start_time
    print('Elapsed {:.2f} min'.format(elapsed/60))
    print('===================================')
    torch.save(model.state_dict(), os.path.join(args.output, 'model-{}.pth'.format(epoch)))
    scheduler.step(epoch)
```
<br>

## 검증

<br>
